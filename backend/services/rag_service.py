# backend/services/rag_service.py
from typing import Dict, Any, List
from backend.core.ports.vector_store import VectorStoreProtocol, RetrievedDocument
from backend.core.ports.llm import LLMProtocol
import logging

logger = logging.getLogger(__name__)


class RAGService:
    def __init__(
        self,
        vector_store: VectorStoreProtocol,
        llm: LLMProtocol
    ):
        self.vector_store = vector_store
        self.llm = llm

    def ask(self, question: str, top_k: int = 3) -> Dict[str, Any]:
        logger.info(f"Processing question: {question[:100]}...")
        
        # Step 1: æ£€ç´¢ç›¸å…³æ–‡æ¡£
        try:
            docs: List[RetrievedDocument] = self.vector_store.similarity_search(question, k=top_k)
        except Exception as e:
            logger.error(f"Vector store search failed: {e}")
            return {
                "answer": "æŠ±æ­‰ï¼Œæ£€ç´¢æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯ã€‚",
                "sources": []
            }

        # Step 2: æ‰¾å‡ºç¬¬ä¸€æ¡éç©ºçš„æœ‰æ•ˆæ–‡æ¡£
        selected_doc = None
        for d in docs:
            if d.content and d.content.strip():
                selected_doc = d
                break

        # Step 3: æ„å»ºä¸Šä¸‹æ–‡ï¼ˆåªç”¨ç¬¬ä¸€æ¡ï¼‰
        if selected_doc:
            # æå–åŸå§‹å†…å®¹
            raw_content = selected_doc.content.strip()
            sources = [{"content": selected_doc.content, "metadata": selected_doc.metadata}]
            
            # ğŸ‘‡ å…³é”®ï¼šå³ä½¿ä¸æˆªæ–­ï¼Œä¹Ÿç¡®ä¿ä¸Šä¸‹æ–‡æ¸…æ™°ï¼ˆå°æ¨¡å‹èƒ½å¤„ç†çŸ­æ–‡æœ¬ï¼‰
            context = f"ã€HRæ”¿ç­–åŸæ–‡ã€‘\n{raw_content}"
        else:
            context = "æ— ç›¸å…³èµ„æ–™ã€‚"
            sources = []

        # Step 4: å¼ºåŒ– system æŒ‡ä»¤ â€”â€” ç²¾å‡†åŒºåˆ†æ¨¡ç³Š vs å…·ä½“é—®é¢˜
        system_message = (
            "ä½ æ˜¯ä¸“ä¸šçš„äººåŠ›èµ„æºåŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹è§„åˆ™å›ç­”ï¼š\n"
            "1. å¦‚æœç”¨æˆ·é—®é¢˜æœªè¯´æ˜å…·ä½“å·¥ä½œå¹´é™ï¼ˆä¾‹å¦‚ï¼šâ€˜å¹´å‡å¤šä¹…ï¼Ÿâ€™ã€â€˜å¹´å‡æœ‰å‡ å¤©ï¼Ÿâ€™ï¼‰ï¼Œ\n"
            "   è¯·å®Œæ•´å›ç­”ï¼šâ€˜å¹´å‡å¤©æ•°æ ¹æ®å·¥é¾„ç¡®å®šï¼šå…¥èŒæ»¡1å¹´ä¸æ»¡10å¹´ä¸º5å¤©ï¼Œæ»¡10å¹´ä¸æ»¡20å¹´ä¸º10å¤©ï¼Œæ»¡20å¹´ä»¥ä¸Šä¸º15å¤©ã€‚â€™\n"
            "2. å¦‚æœç”¨æˆ·æ˜ç¡®æåˆ°å·¥ä½œå¹´é™ï¼ˆä¾‹å¦‚ï¼šâ€˜æˆ‘å·¥ä½œ3å¹´â€™ã€â€˜å…¥èŒ8å¹´â€™ã€â€˜å¹²äº†15å¹´â€™ï¼‰ï¼Œ\n"
            "   è¯·æ ¹æ®æ”¿ç­–åŒ¹é…å¹¶ä»…è¾“å‡ºå¯¹åº”å¤©æ•°ï¼ˆå¦‚â€˜5å¤©â€™ã€â€˜10å¤©â€™ã€â€˜15å¤©â€™ï¼‰ï¼Œä¸è¦è§£é‡Šã€‚\n"
            "3. ç¦æ­¢å›ç­”â€˜æ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•ç¡®å®šâ€™ï¼Œç¦æ­¢éšæ„çŒœæµ‹æˆ–åªé€‰æœ€å¤§å€¼ã€‚\n"
            "4. ä¸å¾—ç¼–é€ æ”¿ç­–ä¸­æ²¡æœ‰çš„å†…å®¹ã€‚"
        )
        user_message = f"å‚è€ƒèµ„æ–™ï¼š\n{context}\n\né—®é¢˜ï¼š{question}"

        # Step 5: è°ƒç”¨ LLM
        try:
            answer = self.llm.generate_with_messages(
                system=system_message,
                user=user_message,
                max_tokens=128,
                temperature=0.0
            )
            # æ¸…ç†å¯èƒ½çš„å¤šä½™æ¢è¡Œæˆ–å‰ç¼€
            answer = answer.strip().split('\n')[0].strip('\"\'')
        except Exception as e:
            logger.error(f"LLM generation failed: {e}")
            answer = "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‘ç”Ÿé”™è¯¯ã€‚"

        return {
            "answer": answer,
            "sources": sources
        }